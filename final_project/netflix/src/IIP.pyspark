from pyspark import SparkContext

Top_N_Similar = 3
Preprocessed_data_path = "file:///home/cloudera/netflix_subset/1.txt"
Original_training_data_path = "file:///home/cloudera/netflix_subset/1.txt"
Testing_data_path = "file:///home/cloudera/netflix_subset/2.txt"

# Start of item-item, Pearson correlation

IIP = sc.textFile(Preprocessed_data_path) \
    .map(lambda line:line.split(',')) \
    .map(lambda splits:(splits[1],(splits[0],float(splits[2])) )) \
    .groupByKey() \
    .mapValues(lambda x: pair(list(x))) \
    .flatMap(lambda x: x[1]) \
    .map(lambda x: (x[0],(x[1][0],x[1][0]*x[1][0],x[1][1],x[1][1]*x[1][1],x[1][0]*x[1][1]))) \
    .groupByKey() \
    .mapValues(lambda x :csim(list(x))) \
    .map(lambda x:(x[1],((x[0][0],x[0][1]),(x[0][1],x[0][0])))) \
    .flatMapValues(lambda x : x) \
    .map(lambda x:(x[1][0],(x[1][1],x[0]))) \
    .groupByKey() \
    .mapValues(lambda x :topN(list(x))) \
    .flatMapValues(lambda x : x) \
    .map(lambda x : (x[0],x[1][0])) 




def pair(x):
	l = []
	
	for i in x:
		for j in x:
			if(j[0]>i[0]):
				l.append(((j[0],i[0]),(j[1],i[1])))

	return l

def csim(x):
	dotProduct = 0
	rating1squaredSum = 0
	rating2squaredSum = 0

	for i in x:
		dotProduct += i[4]
		rating1squaredSum += i[1]
		rating2squaredSum+= i[3]

	return dotProduct/math.sqrt(rating1squaredSum*rating2squaredSum);

def topN(x):
	return sorted(x,key=lambda y: y[1],reverse=True)[:Top_N_Similar]

		
# End of item-item, Pearson correlation


training = sc.textFile(Original_training_data_path) \
    .map(lambda line:line.split(',')) \
    .map(lambda splits:((splits[1],splits[0]),float(splits[2])) ) 



prediction = sc.textFile(Testing_data_path) \
    .map(lambda line:line.split(',')) \
    .map(lambda splits:(splits[0],splits[1]) ) \
    .join(IIP) \
    .map(lambda x: (x[1],x[0])) \
    .join(training) \
    .map(lambda x: ((x[0][0],x[1][0]),x[1][1])) \
    .groupByKey() \
    .mapValues(lambda x: sum(x)/len(x)) \
    .map(lambda x: x[0][1]+","+x[0][0]+","+str(x[1]))

print(prediction.collect())